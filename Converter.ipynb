{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2e796b-e09d-4f21-a527-a96ad36a5173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def convertModel(safetensorFilename, isXL, size, supportsControlnet, isOriginal, bits):\n",
    "    checkpoint_path = safetensorFilename + \".safetensors\"\n",
    "    dump_path = safetensorFilename + \"_diffusers\"\n",
    "    command = f\"python3.10 convert_original_stable_diffusion_to_diffusers.py --checkpoint_path {checkpoint_path} --from_safetensors --device cpu --extract_ema --dump_path {dump_path}\"\n",
    "\n",
    "    if isXL:\n",
    "        command += \" --pipeline_class_name StableDiffusionXLPipeline\"\n",
    "    \n",
    "    print(\"Converting from SD model → Diffusers\")\n",
    "    print(command)\n",
    "    \n",
    "    command = command.split()\n",
    "\n",
    "    subprocess.run(command) \n",
    "    print(\"Finished converting from SD model → Diffusers\\n\\n\")\n",
    "\n",
    "    print(\"Diffusers → MLMODELC\")\n",
    "    command = f\"python3.10 -m python_coreml_stable_diffusion.torch2coreml --latent-w {int(size/8)} --latent-h {int(size/8)} --convert-vae-decoder --convert-vae-encoder --convert-text-encoder --convert-unet --model-version {safetensorFilename}_diffusers --bundle-resources-for-swift-cli\"\n",
    "    \n",
    "    if isXL:\n",
    "        command += \" --xl-version\"\n",
    "\n",
    "    if isOriginal:\n",
    "        command += f\" --attention-implementation ORIGINAL -o {safetensorFilename}_original_{bits}bits_{size}x{size}\"\n",
    "    else:\n",
    "        command += f\" --attention-implementation SPLIT_EINSUM -o {safetensorFilename}_split_einsum_{bits}bits_{size}x{size}\"\n",
    "        \n",
    "    if supportsControlnet:\n",
    "        command += \" --unet-support-controlnet\"\n",
    "\n",
    "    # command += f\" --quantize-nbits {bits}\"\n",
    "\n",
    "    print(f\"Converting: \\n {command}\\n\\n\")\n",
    "\n",
    "    command = command.split()\n",
    "    subprocess.run(command) \n",
    "    \n",
    "    print(\"Finished converting from Diffusers → MLMODELC\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcbcef0-b81d-45d6-9c0b-bfef38371175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting from SD model → Diffusers\n",
      "python3.10 convert_original_stable_diffusion_to_diffusers.py --checkpoint_path realvisxlV30Turbo_v30TurboBakedvae.safetensors --from_safetensors --device cpu --extract_ema --dump_path realvisxlV30Turbo_v30TurboBakedvae_diffusers --pipeline_class_name StableDiffusionXLPipeline\n",
      "Finished converting from SD model → Diffusers\n",
      "\n",
      "\n",
      "Diffusers → MLMODELC\n",
      "Converting: \n",
      " python3.10 -m python_coreml_stable_diffusion.torch2coreml --latent-w 96 --latent-h 96 --convert-vae-decoder --convert-vae-encoder --convert-text-encoder --convert-unet --model-version realvisxlV30Turbo_v30TurboBakedvae_diffusers --bundle-resources-for-swift-cli --xl-version --attention-implementation ORIGINAL -o realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing DiffusionPipeline with realvisxlV30Turbo_v30TurboBakedvae_diffusers..\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:06<00:00,  1.11it/s]\n",
      "INFO:__main__:Done. Pipeline in effect: StableDiffusionXLPipeline\n",
      "INFO:__main__:Attention implementation in effect: AttentionImplementations.ORIGINAL\n",
      "INFO:__main__:Converting vae_decoder\n",
      "/opt/homebrew/lib/python3.10/site-packages/diffusers/models/resnet.py:139: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/opt/homebrew/lib/python3.10/site-packages/diffusers/models/resnet.py:152: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if hidden_states.shape[0] >= 64:\n",
      "/opt/homebrew/lib/python3.10/site-packages/coremltools/models/_deprecation.py:27: FutureWarning: Function _TORCH_OPS_REGISTRY.__contains__ is deprecated and will be removed in 7.2.; Please use coremltools.converters.mil.frontend.torch.register_torch_op\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/homebrew/lib/python3.10/site-packages/coremltools/models/_deprecation.py:27: FutureWarning: Function _TORCH_OPS_REGISTRY.__delitem__ is deprecated and will be removed in 7.2.; Please use coremltools.converters.mil.frontend.torch.register_torch_op\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "INFO:__main__:Converting vae_decoder to CoreML..\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 368/369 [00:00<00:00, 3993.97 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 415.15 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 69/69 [00:00<00:00, 107.76 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 528.62 passes/s]\n",
      "INFO:__main__:Saved vae_decoder into realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_vae_decoder.mlpackage\n",
      "INFO:__main__:Converted vae_decoder\n",
      "INFO:__main__:Converting vae_encoder\n",
      "/opt/homebrew/lib/python3.10/site-packages/diffusers/models/resnet.py:221: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/opt/homebrew/lib/python3.10/site-packages/diffusers/models/resnet.py:226: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "INFO:__main__:Converting vae_encoder to CoreML..\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 295/296 [00:00<00:00, 4480.15 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 542.53 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 69/69 [00:00<00:00, 174.81 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 674.72 passes/s]\n",
      "INFO:__main__:Saved vae_encoder into realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_vae_encoder.mlpackage\n",
      "INFO:__main__:Converted vae_encoder\n",
      "INFO:__main__:Converting unet\n",
      "WARNING:python_coreml_stable_diffusion.unet:`use_linear_projection=True` is ignored!\n",
      "INFO:__main__:Sample UNet inputs spec: {'sample': (torch.Size([2, 4, 96, 96]), torch.float32), 'timestep': (torch.Size([2]), torch.float32), 'encoder_hidden_states': (torch.Size([2, 2048, 1, 77]), torch.float32), 'time_ids': (torch.Size([2, 6]), torch.float32), 'text_embeds': (torch.Size([2, 1280]), torch.float32)}\n",
      "INFO:__main__:JIT tracing..\n",
      "/Users/jbenavidesv/Workspace/ml-stable-diffusion/python_coreml_stable_diffusion/layer_norm.py:61: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert inputs.size(1) == self.num_channels\n",
      "INFO:__main__:Done.\n",
      "INFO:__main__:Converting unet to CoreML..\n",
      "WARNING:coremltools:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/10509 [00:00<?, ? ops/s]WARNING:coremltools:Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "WARNING:coremltools:Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 10507/10509 [00:03<00:00, 2630.09 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:01<00:00,  4.47 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 71/71 [04:46<00:00,  4.04s/ passes]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 35.10 passes/s]\n",
      "INFO:__main__:Saved unet into realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_unet.mlpackage\n",
      "INFO:__main__:Converted unet\n",
      "INFO:__main__:Converting text_encoder\n",
      "INFO:__main__:Sample inputs spec: {'input_ids': (torch.Size([1, 77]), torch.float32)}\n",
      "INFO:__main__:JIT tracing text_encoder..\n",
      "/Users/jbenavidesv/Workspace/ml-stable-diffusion/python_coreml_stable_diffusion/torch2coreml.py:284: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(-1e4, device=device), device=device)\n",
      "/opt/homebrew/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:284: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/opt/homebrew/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:292: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/opt/homebrew/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:324: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "INFO:__main__:Done.\n",
      "INFO:__main__:Converting text_encoder to CoreML..\n",
      "WARNING:coremltools:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/835 [00:00<?, ? ops/s]WARNING:coremltools:Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "WARNING:coremltools:Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 833/835 [00:00<00:00, 8806.14 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 290.21 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 71/71 [00:03<00:00, 18.29 passes/s] \n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 593.15 passes/s]\n",
      "INFO:__main__:Saved text_encoder into realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_text_encoder.mlpackage\n",
      "INFO:__main__:Converted text_encoder\n",
      "INFO:__main__:Converting text_encoder_2\n",
      "INFO:__main__:Sample inputs spec: {'input_ids': (torch.Size([1, 77]), torch.float32)}\n",
      "INFO:__main__:JIT tracing text_encoder_2..\n",
      "INFO:__main__:Done.\n",
      "INFO:__main__:Converting text_encoder_2 to CoreML..\n",
      "WARNING:coremltools:Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/2021 [00:00<?, ? ops/s]WARNING:coremltools:Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  78%|███████▊  | 1578/2021 [00:00<00:00, 6670.27 ops/s]WARNING:coremltools:Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 2019/2021 [00:00<00:00, 6370.70 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 65.32 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 71/71 [00:22<00:00,  3.16 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 188.67 passes/s]\n",
      "INFO:__main__:Saved text_encoder into realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_text_encoder_2.mlpackage\n",
      "INFO:__main__:Converted text_encoder_2\n",
      "INFO:__main__:Bundling resources for the Swift CLI\n",
      "INFO:__main__:Created realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources for Swift CLI assets\n",
      "INFO:__main__:Compiling realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_text_encoder.mlpackage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jbenavidesv/Workspace/ml-stable-diffusion/realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_text_encoder.mlmodelc/coremldata.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Compiled realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_text_encoder.mlpackage to realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/TextEncoder.mlmodelc\n",
      "INFO:__main__:Compiling realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_text_encoder_2.mlpackage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jbenavidesv/Workspace/ml-stable-diffusion/realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_text_encoder_2.mlmodelc/coremldata.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Compiled realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_text_encoder_2.mlpackage to realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/TextEncoder2.mlmodelc\n",
      "INFO:__main__:Compiling realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_vae_decoder.mlpackage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jbenavidesv/Workspace/ml-stable-diffusion/realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_vae_decoder.mlmodelc/coremldata.bin\n",
      "/Users/jbenavidesv/Workspace/ml-stable-diffusion/realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_vae_encoder.mlmodelc/coremldata.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Compiled realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_vae_decoder.mlpackage to realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/VAEDecoder.mlmodelc\n",
      "INFO:__main__:Compiling realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_vae_encoder.mlpackage\n",
      "INFO:__main__:Compiled realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_vae_encoder.mlpackage to realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/VAEEncoder.mlmodelc\n",
      "INFO:__main__:Compiling realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_unet.mlpackage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jbenavidesv/Workspace/ml-stable-diffusion/realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_unet.mlmodelc/coremldata.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Compiled realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_unet.mlpackage to realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Resources/Unet.mlmodelc\n",
      "WARNING:__main__:realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_unet_chunk1.mlpackage not found, skipping compilation to UnetChunk1.mlmodelc\n",
      "WARNING:__main__:realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_unet_chunk2.mlpackage not found, skipping compilation to UnetChunk2.mlmodelc\n",
      "WARNING:__main__:realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_refiner.mlpackage not found, skipping compilation to UnetRefiner.mlmodelc\n",
      "WARNING:__main__:realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_refiner_chunk1.mlpackage not found, skipping compilation to UnetRefinerChunk1.mlmodelc\n",
      "WARNING:__main__:realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_refiner_chunk2.mlpackage not found, skipping compilation to UnetRefinerChunk2.mlmodelc\n",
      "WARNING:__main__:realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_control-unet.mlpackage not found, skipping compilation to ControlledUnet.mlmodelc\n",
      "WARNING:__main__:realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_control-unet_chunk1.mlpackage not found, skipping compilation to ControlledUnetChunk1.mlmodelc\n",
      "WARNING:__main__:realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_control-unet_chunk2.mlpackage not found, skipping compilation to ControlledUnetChunk2.mlmodelc\n",
      "WARNING:__main__:realvisxlV30Turbo_v30TurboBakedvae_original_8bits_768x768/Stable_Diffusion_version_realvisxlV30Turbo_v30TurboBakedvae_diffusers_safety_checker.mlpackage not found, skipping compilation to SafetyChecker.mlmodelc\n",
      "INFO:__main__:Downloading and saving tokenizer vocab.json\n",
      "INFO:__main__:Done\n",
      "INFO:__main__:Downloading and saving tokenizer merges.txt\n",
      "INFO:__main__:Done\n",
      "INFO:__main__:Bundled resources for the Swift CLI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished converting from Diffusers → MLMODELC\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "safetensorFilename = \"realvisxlV30Turbo_v30TurboBakedvae\"\n",
    "isXL = True\n",
    "size = 768\n",
    "supportsControlnet = False\n",
    "isOriginal = True\n",
    "bits = 8\n",
    "\n",
    "convertModel(safetensorFilename, isXL, size, supportsControlnet, isOriginal, bits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
